for( k in seq(1, n)){
for (i in seq(1, m)){
r.denom[k] <- r.denom[k] + pis[i] * dmvnorm(X[k,], mu[i,], diag(sigmas[i,]) )
}
}
#calculate r using the denominator. Stored as a matrix so I can use all the values I want
#would be more optimal to sum once instead of in each loop later but it still runs quickly
for(i in seq(1, m)){
# print(i)
r[,i]<- pis[i] * dmvnorm(X, mu[i,], diag(sigmas[i,]) ) / r.denom
}
# MMMMMMMMMM
#calculate pi, means, and sigma
for (i in seq(1, m)){
pis[i]<-sum(r[,i])/n
mu[i,] <- cust.sum(as.vector(r[,i]) * X)/ sum(r[,i])
sigmas[i,] <- cust.sum(r[,i] * (X-mu[i,])^2 )  / sum(r[,i])
}
pis = normalize(pis)
#loop to calculate the likelihood using the likelihood function
total.likelihood <- 0
for (i in seq(1, m)){
total.likelihood = total.likelihood + sum(r[,i] * (log(pis[i]) + log(dmvnorm(X, mu[i,], diag(sigmas[i,]) ) )))
}
l[count+1] <- total.likelihood
count = count+1 #update number of iterations
}
#returns the means, sigma, and priors (weights)
return( list(mu, sigmas, pis, total.likelihood) )
}
data<-readRDS("Project.Mixture.Data")
X=cbind(data[1,], data[2,])
input.mu = matrix( seq(1:6),ncol = 2)
input.sigmas = matrix( seq(1:6),ncol = 2)
input.pis = normalize(seq(1:3))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
graphing(means, sigmas, pis)
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
AIC(length(means) + length(sigmas) + length(pis), l )
?plot
graphing <- function(means, sigmas, pis){
#generate points for the density plot
results = c()
nsize=10000
s = table( sample(seq(length(pis)), size= nsize, replace=TRUE, prob=pis) )
for (i in seq(length(means[,1]))  ) {
results = append(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
}
d = density(results)
plot(d, main="Distribution estimate")
}
graphing(means, sigmas, pis)
graphing <- function(means, sigmas, pis){
#generate points for the density plot
results = c()
nsize=10000
s = table( sample(seq(length(pis)), size= nsize, replace=TRUE, prob=pis) )
for (i in seq(length(means[,1]))  ) {
results = append(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
}
d = density(results)
plot(d, main="EM distribution estimate")
}
graphing(means, sigmas, pis)
res
length(means)
X=cbind(data[1,], data[2,])
input.mu = matrix( seq(1:8),ncol = 2)
input.sigmas = matrix( seq(1:8),ncol = 2)
input.pis = normalize(seq(1:4))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
graphing(means, sigmas, pis)
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
AIC(length(means) + length(sigmas) + length(pis), l )
BIC <- function(k, n, l)
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
BIC <- function(k, n, l)
BIC <- function(k, n, l){
return(k * log(n) - 2 * l)
}
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
res
data<-readRDS("Project.Mixture.Data")
X=cbind(data[1,], data[2,])
input.mu = matrix( seq(1:4),ncol = 2)
input.sigmas = matrix( seq(1:4),ncol = 2)
input.pis = normalize(seq(1:2))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
graphing(means, sigmas, pis)
#already calculating the log likelyhood in the EM algorithm so I can just use that value
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
BIC <- function(k, n, l){
return(k * log(n) - 2 * l)
}
AIC(length(means) + length(sigmas) + length(pis), l )
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
res
data<-readRDS("Project.Mixture.Data")
X=cbind(data[1,], data[2,])
input.mu = matrix( seq(1:2),ncol = 2)
input.sigmas = matrix( seq(1:2),ncol = 2)
input.pis = normalize(seq(1:1))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
graphing(means, sigmas, pis)
#already calculating the log likelyhood in the EM algorithm so I can just use that value
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
BIC <- function(k, n, l){
return(k * log(n) - 2 * l)
}
AIC(length(means) + length(sigmas) + length(pis), l )
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
res
data<-readRDS("Project.Mixture.Data")
X=cbind(data[1,], data[2,])
input.mu = matrix( seq(1:6),ncol = 2)
input.sigmas = matrix( seq(1:6),ncol = 2)
input.pis = normalize(seq(1:3))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
graphing(means, sigmas, pis)
#already calculating the log likelyhood in the EM algorithm so I can just use that value
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
BIC <- function(k, n, l){
return(k * log(n) - 2 * l)
}
AIC(length(means) + length(sigmas) + length(pis), l )
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
length(means) + length(sigmas) + length(pis)
matrix( c(-7, 1, 6, -1, 1, 1),ncol = 2)
matrix( c(15, 15, 15, 15, 15, 30),ncol = 2)
input.mu = matrix( c(-7, 1, 6, -1, 1, 1),ncol = 2)
input.sigmas = matrix( c(15, 15, 15, 15, 15, 30),ncol = 2)
input.pis = normalize(c(1,1,1))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
graphing(means, sigmas, pis)
#already calculating the log likelyhood in the EM algorithm so I can just use that value
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
BIC <- function(k, n, l){
return(k * log(n) - 2 * l)
}
AIC(length(means) + length(sigmas) + length(pis), l )
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
res
X=cbind(data[1,], data[2,])
input.mu = matrix( c(-7, 1, 1, 1),ncol = 2)
input.sigmas = matrix( c(15, 15, 15, 30),ncol = 2)
input.pis = normalize(c(1,1))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
graphing(means, sigmas, pis)
res
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
BIC <- function(k, n, l){
return(k * log(n) - 2 * l)
}
AIC(length(means) + length(sigmas) + length(pis), l )
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
X=cbind(data[1,], data[2,])
input.mu = matrix( c(1, 1),ncol = 2)
input.sigmas = matrix( c(15, 15),ncol = 2)
input.pis = normalize(c(1))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
graphing(means, sigmas, pis)
#already calculating the log likelyhood in the EM algorithm so I can just use that value
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
BIC <- function(k, n, l){
return(k * log(n) - 2 * l)
}
AIC(length(means) + length(sigmas) + length(pis), l )
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
data<-readRDS("Project.Mixture.Data")
X=cbind(data[1,], data[2,])
input.mu = matrix( c(-7, 1, 6, 0 -1, 1, 1, 0),ncol = 2)
input.sigmas = matrix( c(15, 15, 15,15, 15, 15, 15, 30),ncol = 2)
input.pis = normalize(c(1,1,1,1))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
graphing(means, sigmas, pis)
#already calculating the log likelyhood in the EM algorithm so I can just use that value
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
BIC <- function(k, n, l){
return(k * log(n) - 2 * l)
}
AIC(length(means) + length(sigmas) + length(pis), l )
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
data<-readRDS("Project.Mixture.Data")
X=cbind(data[1,], data[2,])
input.mu = matrix( c(-7, 1, 6, 0, -1, 1, 1, 0),ncol = 2)
input.sigmas = matrix( c(15, 15, 15,15, 15, 15, 15, 30),ncol = 2)
input.pis = normalize(c(1,1,1,1))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
graphing(means, sigmas, pis)
#already calculating the log likelyhood in the EM algorithm so I can just use that value
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
BIC <- function(k, n, l){
return(k * log(n) - 2 * l)
}
AIC(length(means) + length(sigmas) + length(pis), l )
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
#generate points for the density plot
results = c()
nsize=10000
s = table( sample(seq(length(pis)), size= nsize, replace=TRUE, prob=pis) )
rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )
rmvnorm(s[names(s)==1], means[1,], diag(sigmas[1,]) )
for (i in seq(length(means[,1]))  ) {
results = c(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
}
results
results = cbind(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
#generate points for the density plot
results = matrix()
s = table( sample(seq(length(pis)), size= nsize, replace=TRUE, prob=pis) )
for (i in seq(length(means[,1]))  ) {
results = cbind(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
}
rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )
list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))
list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ), rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))
list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
c(list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )), list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )))
do.call(c, list(list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )), list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))))
do.call(c, list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )), list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )))
do.call(c, list(list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )), list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))))
append(list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )), list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )))
append(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ), rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))
list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))
list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))[1,]
list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))[,1]
list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))[1]
list(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))[[1]]
vector(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ))
matrix(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )), ncol = 2)
matrix(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ), ncol = 2)
rbind(rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ), rmvnorm(s[names(s)==1], means[1,], diag(sigmas[1,]) ))
graphing <- function(means, sigmas, pis){
#generate points for the density plot
results = matrix()
nsize=10000
s = table( sample(seq(length(pis)), size= nsize, replace=TRUE, prob=pis) )
for (i in seq(length(means[,1]))  ) {
if (i==1){
results = rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )
}
else{
results = rbind(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
}
}
d = density(results)
plot(d, main="EM distribution estimate")
}
graphing(means, sigmas, pis)
for (i in seq(length(means[,1]))  ) {
if (i==1){
results = rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )
}
else{
results = rbind(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
}
}
graphing <- function(means, sigmas, pis){
#generate points for the density plot
results = matrix()
nsize=10000
s = table( sample(seq(length(pis)), size= nsize, replace=TRUE, prob=pis) )
for (i in seq(length(means[,1]))  ) {
if (i==1){
results = rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )
}
else{
results = rbind(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
}
}
# d = density(results)
# plot(d, main="EM distribution estimate")
}
graphing(means, sigmas, pis)
library(MASS)
results = matrix()
nsize=10000
s = table( sample(seq(length(pis)), size= nsize, replace=TRUE, prob=pis) )
for (i in seq(length(means[,1]))  ) {
if (i==1){
results = rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )
}
else{
results = rbind(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
}
}
results
nsize=10000
s = table( sample(seq(length(pis)), size= nsize, replace=TRUE, prob=pis) )
for (i in seq(length(means[,1]))  ) {
if (i==1){
results = rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )
}
else{
results = rbind(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
}
}
#generate points for the density plot
results
#generate points for the density plot
nsize=10000
s = table( sample(seq(length(pis)), size= nsize, replace=TRUE, prob=pis) )
results = rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )
results = rbind(results, rmvnorm(s[names(s)==1], means[1,], diag(sigmas[1,]) ) )
results
results[1,]
results[,1]
den3d <- kde2d(results[,1], results[,2])
persp(den3d, box=FALSE)
?persp
persp(d, box=TRUE)
d <- kde2d(results[,1], results[,2])
persp(d, box=TRUE)
view3d(theta = 20, phi = 60)
library(rgl)
persp(d, box=TRUE)
persp(d, xlab=x, ylab=y, box=TRUE)
persp(d, xlab="x", ylab="y", box=TRUE)
install.packages(rgl)
install.packages("rgl")
library(rgl)
view3d(theta = 30, phi=15)
view3d(theta = 20, phi=20)
rgl.printRglwidget = TRUE)
rgl.printRglwidget = TRUE
view3d(theta = 20, phi=20
# d = density(results)
# plot(d, main="EM distribution estimate")
}
em <- function(X, input.mu, input.sigmas, input.pis, epsilon)
{
#initialize the needed variables
n = length(X[,1])
m = length(input.mu[,1])
pis = input.pis
mu = input.mu
sigmas = input.sigmas
r = matrix(1, ncol=m, nrow=n)
#make a backup stopping criteria
count <- 2
#make a list to hold all my likelihood estimates and set first position as something larger than 0 so doesn't break first iteration
l <- rep(0, 500)
l[count] <- 10
#iterate while less than max iterations and likelihood is still changing
while(count < 400 & (abs(l[count] - l[count-1]) > epsilon))
{
# EEEEEEEEEE
#calculate the denominator for r first
r.denom = rep(0, n)
for( k in seq(1, n)){
for (i in seq(1, m)){
r.denom[k] <- r.denom[k] + pis[i] * dmvnorm(X[k,], mu[i,], diag(sigmas[i,]) )
}
}
#calculate r using the denominator. Stored as a matrix so I can use all the values I want
#would be more optimal to sum once instead of in each loop later but it still runs quickly
for(i in seq(1, m)){
# print(i)
r[,i]<- pis[i] * dmvnorm(X, mu[i,], diag(sigmas[i,]) ) / r.denom
}
# MMMMMMMMMM
#calculate pi, means, and sigma
for (i in seq(1, m)){
pis[i]<-sum(r[,i])/n
mu[i,] <- cust.sum(as.vector(r[,i]) * X)/ sum(r[,i])
sigmas[i,] <- cust.sum(r[,i] * (X-mu[i,])^2 )  / sum(r[,i])
}
pis = normalize(pis)
#loop to calculate the likelihood using the likelihood function
total.likelihood <- 0
for (i in seq(1, m)){
total.likelihood = total.likelihood + sum(r[,i] * (log(pis[i]) + log(dmvnorm(X, mu[i,], diag(sigmas[i,]) ) )))
}
l[count+1] <- total.likelihood
count = count+1 #update number of iterations
}
#returns the means, sigma, and priors (weights)
return( list(mu, sigmas, pis, total.likelihood) )
}
data<-readRDS("Project.Mixture.Data")
X=cbind(data[1,], data[2,])
input.mu = matrix( c(-7, 1, 6, 0, -1, 1, 1, 0),ncol = 2)
input.sigmas = matrix( c(15, 15, 15,15, 15, 15, 15, 30),ncol = 2)
input.pis = normalize(c(1,1,1,1))
epsilon = 1
res = em(X, input.mu, input.sigmas, input.pis, epsilon)
res
means = matrix(unlist(res[1]),ncol = 2)
sigmas = matrix(unlist(res[2]),ncol = 2)
pis = normalize(unlist(res[3]))
l = unlist(res[4])
library(rgl)
library(MASS)
graphing(means, sigmas, pis)
#already calculating the log likelyhood in the EM algorithm so I can just use that value
AIC <- function(num.params, likelyhood){
return(2 * num.params - 2 * likelyhood)
}
BIC <- function(k, n, l){
return(k * log(n) - 2 * l)
}
AIC(length(means) + length(sigmas) + length(pis), l )
BIC(length(means) + length(sigmas) + length(pis), length(X[,1]), l)
view3d(theta = 20, phi=20)
view3d(theta = 40, phi=20)
d <- kde2d(results[,1], results[,2])
persp(d, xlab="x", ylab="y", box=TRUE)
view3d(theta = 40, phi=20)
options(rgl.printRglwidget = TRUE)
view3d(theta = 40, phi=20)
options(rgl.printRglwidget = TRUE)
d <- kde2d(results[,2], results[,1])
persp(d, xlab="y", ylab="x", box=TRUE)
d <- kde2d(results[,1], results[,2])
persp(d, xlab="y", ylab="x", box=TRUE)
generate points for the density plot
nsize=10000
s = table( sample(seq(length(pis)), size= nsize, replace=TRUE, prob=pis) )
for (i in seq(length(means[,1]))  ) {
if (i==1){
results = rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) )
}
else{
results = rbind(results, rmvnorm(s[names(s)==i], means[i,], diag(sigmas[i,]) ) )
}
}
d <- kde2d(results[,1], results[,2])
persp(d, xlab="y", ylab="x", box=TRUE)
d <- kde2d(results[,2], results[,1])
persp(d, xlab="y", ylab="x", box=TRUE)
d <- kde2d(results[,1], results[,2])
persp(d, xlab="x", ylab="y", box=TRUE)
